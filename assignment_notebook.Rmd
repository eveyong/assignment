---
title: "Housing in London"
author: "Wen Qian Yong"
date: "May 2021"
header-includes: 
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{Wen Qian Yong}
- \fancyfoot[C]{Housing in London}
- \fancyfoot[R]{\thepage}
subtitle: "What contributes to London's house prices?"
abstract: "The Kaggle dataset used here is primarily centered around the housing market of London. It contains a lot of additional relevant data, such as average house prices, salary, percentage of households that recycle, life satisfaction, number of jobs, area size in hectares, number of people living in the area, and the number of crimes committed. The dataset is used to analyse how London's house prices change over time and what contributes to different house prices in different boroughs, then ultimately predict future house prices using machine learning techniques."
keywords: "House prices, prediction, machine learning, data analytics"
output: 
  pdf_document:
    toc: true
    
---

```{r global-options, include=FALSE}
#install.packages("fancyhdr")
#library(fancyhdr)
#install.packages("knitr")

# setting global options for the code to not show in PDF, but these will be picked up in appendix via another code
library(knitr) 
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)

```
\newpage

## Introduction

The dataset chosen is the "Housing in London 2020", found on Kaggle (https://www.kaggle.com/justinas/housing-in-london), split by areas of London called boroughs (a flag exists to identify these). This dataset contains a lot of additional relevant data, such as average house prices, salary, percentage of households that recycle, life satisfaction, number of jobs, area size in hectares, number of people living in the area, and the number of crimes committed.The data is split into two files based on the variable collection frequency (monthly and yearly).

The purpose of choosing this dataset is to analyse how London's house prices change over time and what contributes to different house prices in different boroughs. There are several machine learning algorithms used to predict London's house prices and results are compared to determine which alhorithm works better for this dataset.

The report includes:

* Data cleaning and preparation
* Data exploration and visualisation
* Modelling, prediction, and evaluation
* Conclusion
* R code in the appendices




## Data Cleaning and Preparation

There are two sets of data - monthly and yearly. We take a look at each one of them to find out what information is available and check for duplicates and missing values. Then we determine which variables or columns are least useful (so we drop them) or which ones require modifying for our analysis.


```{r}
#clearing out the environment first
rm(list=ls()) 

#importing the data from monthly file first
data_monthly_raw <- read.csv("1_data/housing_in_london_monthly_variables.csv", sep = ",", header = TRUE)
str(data_monthly_raw)

```
```{r}
#pushing this to a separate r line because it could not fit the frame with the structure above
head(data_monthly_raw)
```

The monthly data contains 13,549 rows and 7 columns. This data also contains the "average price" variable that we need to predict by the end of the report. It also contains the code and area, which could be useful for matching against the yearly data. There is also crime information, which might be useful to predict house prices. 

Checking for missing values.

```{r}
#install.packages("psych")
#library(psych)
#describe(data_monthly_raw)


#install.packages("naniar")
library(naniar)
miss_var_summary(data_monthly_raw)

```

From the table above, we can see that a very small percentage of "houses sold" is missing values. The field "no_of_crimes" appears to be 45% missing from the data, which seems to be a concern for the analysis. 

```{r}

#install.packages("ggplot2")
library(ggplot2)
library(dplyr)

#introduce the year and month columns rather than relying on the date column
data_monthly <- data_monthly_raw %>% 
  mutate(year = as.numeric(substr(date,1,4))) %>% 
  mutate(month = as.numeric(substr(date,6,7)))

#checking the crime column to see why so many are missing
data_monthly %>% 
  ggplot(aes(year, no_of_crimes, col = borough_flag)) +
  geom_point(show.legend = T) 
#looks like it is only introduced in 2001


```

Figure X above shows that the crime data was only introduced in 2001, and that it only exists for data within London's boroughs (those with the flag =1), which is the main focus of the data analysis. No further action on the crime column for now until we join the monthly data with the yearly data. 

The following data manipulations are performed to the monthly data:

* Introduce two additional columns called "year" and "month" to represent the year and month of the data because it will make joining to the yearly data file easier;
* Replace the missing values of "houses sold" with the mean value because so few are missing, it would not make a huge difference what it is replaced by, but getting rid of the missing values will be useful for data analysis further down the track; and
* Filter for boroughs in London only as that is the target market we are interested in only, this reduces the number of rows to 9,936.


```{r}
data_monthly <- data_monthly_raw %>% 
  filter(borough_flag == 1) %>%
  mutate(year = as.numeric(substr(date,1,4))) %>% #to preserve the previous manipulations
  mutate(month = as.numeric(substr(date,6,7))) %>% #to preserve the previous manipulations
  mutate(houses_sold = ifelse(is.na(houses_sold), mean(houses_sold,na.rm=TRUE), houses_sold))
  
#checking for missing values
miss_var_summary(data_monthly)
```

Next, we explore the yearly data.

```{r}
#importing the data from the yearly file
data_yearly_raw <- read.csv("1_data/housing_in_london_yearly_variables.csv", sep = ",", header = TRUE)
str(data_yearly_raw)
head(data_yearly_raw)

```

The yearly data is more generous, it contains more factors that could contribute to average house prices, e.g. life satisfaction, area size, population, salary, etc. As pointed out in the monthly data, we see that "code" and "area" are the common variables / columns among the two data files.


```{r}
#describe(data_yearly)
miss_var_summary(data_yearly_raw) 

```
Like the monthly data, there seems to be some missing variables in the yearly data, most of which can be manipulated to populate some data. The field "life satisfaction" is almost 70% missing from the data, so we might choose to drop that field altogether. Before we do that, we look at the distribution of life satisfaction, area size, number of houses, and number of jobs to see why there are so many missing values.


```{r}

#introduce the year column for easy plotting and only filter for London boroughs
data_yearly <- data_yearly_raw %>% 
  filter(borough_flag == 1) %>%
  mutate(year = as.numeric(substr(date,1,4)))

p1 <- data_yearly %>% 
        ggplot(aes(year, life_satisfaction)) +
        geom_point(color = "blue") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p2 <- data_yearly %>% 
        ggplot(aes(year, area_size)) +
        geom_point(color = "dark green") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p3 <- data_yearly %>% 
        ggplot(aes(year, no_of_houses)) +
        geom_point(color = "maroon") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p4 <- data_yearly %>% 
        ggplot(aes(year, number_of_jobs)) +
        geom_point(color = "orange") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p5 <- data_yearly %>% 
        ggplot(aes(year, population_size)) +
        geom_point(color = "mistyrose4") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p6 <- data_yearly %>% 
        ggplot(aes(year, median_salary)) +
        geom_point(color = "plum4") +
        theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),"cm"))

#use gridExtra for arranging the graphs on one panel
#install.packages("gridExtra")
library(gridExtra)
#install.packages("grid")
library(grid)

grid.arrange(
  p1,
  p2,
  p3,
  p4,
  p5,
  p6,
  nrow = 3.5,
  top = "Why are there missing values from these variables?",
  bottom = textGrob(
    "filtered for areas in London boroughs",
    gp = gpar(fontface = 3, fontsize = 9),
    hjust = 1,
    x = 1
  )
)

```

The reason why life satisfaction has so many missing variables is because the data collection did not start until past 2010. While we may not use this in our machine learning techniques, we could still choose to separate the analysis into post-2010 with life satisfaction added as another factor.

For the other missing variables, we make the following observations:
* population size, number of jobs, and number of houses for 2019 is not populated
* area size is constant and only populated from 2001 to 2018
* median salary is populated since 1999


```{r, include = FALSE}
#don't need to include the output from here

#need the dplyr for data manipulation
#install.packages("dplyr")
library(dplyr)

data_yearly <- data_yearly_raw %>% 
  filter(borough_flag == 1) %>%
  mutate(year = as.numeric(substr(date,1,4))) %>% 
  mutate(month = as.numeric(substr(date,6,7)))  

#the following code populates area size for each borough according to its mean (constant number)
#also the same procedure for median salary
#install.packages("data.table")
library(data.table)
library(plyr); library(dplyr)
setDT(data_yearly)
data_yearly[, area_size := impute_mean(area_size), by = area][, median_salary := impute_mean(median_salary), by = area]
data_yearly %>%
    group_by(area) %>%
    mutate(
        area_size = impute_mean(area_size),
        median_salary = impute_mean(median_salary)
    )

as.data.frame(data_yearly)
```

The following data manipulations are performed to the yearly data:

* Similarly to monthly data, introduce two additional columns called "year" and "month" to represent the year and month of the data
* Replace the missing values of median salary with its mean values because the percentages missing are low; 
* Replace the missing values of area size with its the associated borough's area size as these area sizes do not change overtime - they have been set in stone since the beginning; and
* Filter for boroughs in London only as that is the target market we are interested in only, this reduces the number of rows to 693.

```{r}
#checking for missing variables again after filtering and manipulating
miss_var_summary(data_yearly)
```
### Joining the two data files into one

The variable we are interested in predicting is the "average house price", which is contained in the monthly data. However, there are more attributes in the yearly data, so the final data table should be the yearly data plus the attributes from the monthly data. The yearly data has 693 observations, made up of data from 33 boroughs over 21 years.

```{r}

#checking that the number of observations are truly unique in the yearly data
#cat("number of unique rows in yearly data = ", nrow(unique(data_yearly[c("year","month","area")])), "\n") #693 rows, so yes, all year/month/area is unique

#use left join to join the monthly data onto the yearly data
#join on year, month, and area
data_join <- left_join(data_yearly, data_monthly, by = c("year", "month", "area"))

#do the same check for unique values in final dataset after join
#cat("number of unique rows in final data = ", nrow(unique(data_final[c("year","month","area")])), "\n")

```
A preview of the final data table is shown below.

```{r}
head(data_join)
```


\newpage
## Exploratory Data Analysis

The dataset now has average house prices for 33 boroughs since 1999 until 2019 (21 years), along with the following 9 attributes:
```{r}
#create matrix with 3 columns
tab <- matrix(c("median salary", "-",
                "mean salary", "-",
                "percentage of recycling houses", "-",
                "population size", "data collected until 2018 only",
                "number of jobs", "(data collected until 2018 only)",
                "number of houses sold", "(data collected until 2018 only)",
                "area size", "(data collected from 2001 only)",
                "number of crimes", "(data collected from 2001 only)",
                "life satisfaction", "(data collected from 2011 only)"), 
              ncol=2, byrow=TRUE)

#define column names and row names of matrix
colnames(tab) <- c("Factor or attribute", "Comments on missing variables")
rownames(tab) <- c("1.", "2.", "3.", "4.", "5.", "6.", "7.", "8.", "9.")

#convert matrix to table 
tab <- as.table(tab)

#view table 
tab

```
The 33 boroughs are shown below. 

```{r, include=FALSE}
print(unique((data_join$area)))

```

The graph below shows how average house prices have changed from 1999 to 2019. Average house prices have increased for all areas in general, although some areas have had more dramatic increases compared to others. There is a slight dip in average house prices around 2008/2009, which could be due to the GFC. An indicator that is closely related to the GFC is mean and median salary. We will have a look at the correlation of salary with average house prices later on.

```{r, echo=TRUE, fig.height=5, fig.width=10}
data_join %>% 
  ggplot(aes(year,average_price, col=area)) +
  geom_line() +
  theme(legend.position = "bottom") + 
  theme(legend.key.height = unit(0.3, "cm")) +
  theme(plot.title = element_text(face="bold", hjust=0.5)) +
  theme(plot.subtitle = element_text(hjust=0.5)) +
  theme(plot.margin=unit(c(0.2,0.5,0.1,0.5),"cm")) +
  labs(title="Average House Prices from 1999 to 2019 by Area", subtitle = "Overall increase except in year 2008", x ="Year", y="Average house price (£)") +
  scale_y_continuous(
  labels = scales::comma_format(big.mark = ',',
                                decimal.mark = '.')) +
  scale_x_continuous(breaks = c(1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)) +
  geom_vline(xintercept=2008, linetype = "dashed", col = "black")

  
```


```{r, include = FALSE}

data_join %>% 
  ggplot(aes(year,average_price, col = area)) +
  geom_line(show.legend = FALSE) +
  facet_wrap(~ area)

```

The diagram below shows that Kensington and Chelsea and Westminister have the highest average house prices in London, followed by Camden, Hammersmith, and London. Area seems to be a strong indicator to whether the average house price is high or not.

```{r, echo=TRUE, fig.height=6 fig.width=12}

#install.packages("scales")
library(scales)
data_join %>% 
  ggplot(aes(x=reorder(area, -average_price), y=average_price, col = area)) +
  #ggplot(aes(x=area, y=average_price, col = area)) +
  geom_boxplot(show.legend = FALSE) +
  theme(axis.text.x = element_text(angle=90, hjust=1)) +
  theme(plot.title = element_text(face="bold", hjust=0.5)) +
  theme(plot.subtitle = element_text(hjust=0.5)) +
  theme(plot.margin=unit(c(0.2,0.5,0.1,0.5),"cm")) +
  labs(title="Distribution of average house prices", subtitle = "How are average prices different in each area?", x ="Area", y="Average house price (£)") +
  scale_y_continuous(
  labels = scales::comma_format(big.mark = ',',
                                decimal.mark = '.'))
  
```






\newpage
## Factors Contributing to House Prices

<Refine my question>

We know that area (i.e. location of the house) is a very important factor. However, area or location is not an attribute on its own, but rather the makeup of several attributes in the data. For example:
* number of crimes is a proxy for safety, 
* median and mean salary is a proxy for income and income inequality
* population size and area size is a proxy for population density
* percentage of recylcing is a proxy for social policy


_Location_ (and the above attributes) drive the demand for housing in a particular area, and therefore the average house price.

To help us figure out which attributes are most important, and therefore predict the average house price, the following factors are introduced:
* distance from CBD (from new dataset)
* population density (population size divided by area size)

```{r}

#installing packages required for showing map 

#install.packages("tmap")
#install.packages("sf")
#install.packages("raster")
#install.packages("spData")
#install.packages("spDataLarge", repos = "https://nowosad.github.io/drat/", type = "source")

library(spData)
library(tmap)
library(sf)
library(spData)
library(spDataLarge)
library(tidyverse)



#tm_shape(london) +
 # tm_fill() +
#  tm_borders() 

```



Since majority of the data are complete as at 2018, we will only consider analysis on the data from 1999 to 2018, and then separately with life satisfaction from 2011 to 2018. 




Insights


```{r, include=FALSE}
#need the corrplot package for correlation visualisation
#install.packages("corrplot")
library(corrplot)

#first need to get out the numerical variables only
data_corr <- data_join %>% 
  filter(year>2001, year<2019) %>% 
  mutate(average_price = as.numeric(average_price), 
         median_salary = as.numeric(median_salary), 
         mean_salary = as.numeric(mean_salary),
         life_satisfaction = as.numeric(life_satisfaction), 
         recycling_pct = as.numeric(recycling_pct), 
         population_size = as.numeric(population_size), 
         number_of_jobs = as.numeric(number_of_jobs), 
         area_size = as.numeric(area_size), 
         no_of_crimes = as.numeric(no_of_crimes), 
         no_of_houses = as.numeric(no_of_houses),
         houses_sold = as.numeric(houses_sold)) %>%
  select(average_price, 
         median_salary, 
         mean_salary,
         life_satisfaction, 
         recycling_pct, 
         population_size, 
         number_of_jobs, 
         area_size, 
         no_of_crimes, 
         no_of_houses,
         houses_sold) 

str(data_corr) #all numerical
miss_var_summary(data_corr) #need to remove the missing values otherwise correlation matrix does not work

#since there are not many (only 8) missing, we remove the NAs

data_corr_complete <- na.omit(data_corr)
corr <- cor(data_corr_complete)


```


```{r}
#combine the correlation matrix with significance test
#install.packages("Hmisc")
library(Hmisc)
cor_5 <- rcorr(as.matrix(data_corr_complete))
M <- cor_5$r 
p_mat <- cor_5$P #p-value

#then plot the correlation matrix

corrplot(M, method = "circle", type = "lower",  #type of correlation visualisation, only want one half of it
         tl.col = "darkblue", tl.srt = 65, #change colour of text and slant it
         tl.cex = 0.7, cl.cex = 0.7,
         p.mat = p_mat, sig.level = 0.001, insig = "blank", #remove those insignificant ones, use 0.001 as the level of significance 
         mar=c(0,0,3,0),
         title = "How do the variables relate average house price and each other?")

```
From the correlation matrix above, we make the following observations:
* Average price is positively correlated with salary, number of jobs, life satisfaction (unsurprising);
* Number of jobs in the area also increase the average house price (possibly because commute is convenient);
* Number of crimes increase where there is higher number of jobs so average house prices also increase where there are higher number of crime; and
* Average house price is lower when population size and area size are bigger. Area size is negatively correlated with number of jobs, which implies that these areas have large lands, but fewer job opportunities, hence lower average house prices.



### Modelling 1



### Modelling 2
The following correlation plot shows that:
XXXXXXXXXXX










\newpage
## Evaluation of Models





\newpage
## Conclusion




\newpage
## Limitations and Potential Future Work


\newpage
## Appendices

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```

