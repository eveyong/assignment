---
title: "Housing in London"
author: "Wen Qian Yong"
date: "May 2021"
header-includes: 
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{Wen Qian Yong}
- \fancyfoot[C]{Housing in London}
- \fancyfoot[R]{\thepage}
subtitle: "What contributes to London's house prices?"
abstract: "The Kaggle dataset used here is primarily centered around the housing market of London. It contains a lot of additional relevant data, such as average house prices, salary, percentage of households that recycle, life satisfaction, number of jobs, area size in hectares, number of people living in the area, and the number of crimes committed. The dataset is used to analyse how London's house prices change over time and what contributes to different house prices in different boroughs, then ultimately predict future house prices using machine learning techniques."
keywords: "House prices, prediction, machine learning, data analytics"
output: 
  pdf_document:
    toc: true
    
---

```{r global-options, include=FALSE}
#install.packages("fancyhdr")
#library(fancyhdr)
#install.packages("knitr")

# setting global options for the code to not show in PDF, but these will be picked up in appendix via another code
library(knitr) 
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)

```

\newpage

## Introduction

The dataset chosen is the "Housing in London 2020", found on Kaggle (https://www.kaggle.com/justinas/housing-in-london), split by areas of London called boroughs (a flag exists to identify these). This dataset contains a lot of additional relevant data, such as average house prices, salary, percentage of households that recycle, life satisfaction, number of jobs, area size in hectares, number of people living in the area, and the number of crimes committed.The data is split into two files based on the variable collection frequency (monthly and yearly).

The purpose of choosing this dataset is to analyse how London's house prices change over time and what contributes to different house prices in different boroughs. There are several machine learning algorithms used to predict London's house prices and results are compared to determine which alhorithm works better for this dataset.

Predicting the value of house prices is useful for many parties like investors, banks, insurers, the government, and many more. Managing the risk of fluctuations in house prices helps manage the financial risks for these stakeholders due to their asset values, loan values, insurance payout (property losses), and affordability of house prices.

The report includes:

* Data cleaning and preparation
* Data exploration and visualisation
* Modelling, prediction, and evaluation
* Conclusion
* Appendix for R code

## Data Cleaning and Preparation

There are two sets of data - monthly and yearly. We take a look at each one of them to find out what information is available and check for duplicates and missing values. Then we determine which variables or columns are least useful (so we drop them) or which ones require modifying for our analysis.


```{r}
#clearing out the environment first
rm(list=ls()) 

#importing the data from monthly file first
data_monthly_raw <- read.csv("1_data/housing_in_london_monthly_variables.csv", sep = ",", header = TRUE)

str(data_monthly_raw)

```
```{r}
#pushing this to a separate r line because it could not fit the frame with the structure above
head(data_monthly_raw)

```

The monthly data contains 13,549 rows and 7 columns. This data also contains the "average price" variable that we need to predict by the end of the report. It also contains the code and area, which could be useful for matching against the yearly data. There is also crime information, which might be useful to predict house prices. 

Checking for missing values.

```{r}
#install.packages("psych")
#library(psych)
#describe(data_monthly_raw)


#install.packages("naniar")
library(naniar)
miss_var_summary(data_monthly_raw))

```

From the table above, we can see that a very small percentage of "houses sold" is missing values. The field "no_of_crimes" appears to be 45% missing from the data, which seems to be a concern for the analysis. 

```{r, warning = FALSE}

#install.packages("ggplot2")
library(ggplot2)
library(dplyr)

#introduce the year and month columns rather than relying on the date column
data_monthly <- data_monthly_raw %>% 
  mutate(year = as.numeric(substr(date,1,4))) %>% 
  mutate(month = as.numeric(substr(date,6,7)))

#checking the crime column to see why so many are missing
data_monthly %>% 
  ggplot(aes(year, no_of_crimes, col = borough_flag)) +
  geom_point(show.legend = T) +
  ggtitle("What is going on with the crime data?", subtitle = "Crime data was introduced only in 2001") +
  theme(plot.title = element_text(face="bold", hjust=0.5)) +
  theme(plot.subtitle = element_text(hjust=0.5)) 
#looks like it is only introduced in 2001


```

The figure above shows that the crime data was only introduced in 2001, and that it only exists for data within London's boroughs (those with the flag =1), which is the main focus of the data analysis. No further action on the crime column for now until we join the monthly data with the yearly data. 

The following data manipulations are performed to the monthly data:

* Introduce two additional columns called "year" and "month" to represent the year and month of the data because it will make joining to the yearly data file easier;
* Replace the missing values of "houses sold" with the mean value because so few are missing, it would not make a huge difference what it is replaced by, but getting rid of the missing values will be useful for data analysis further down the track; and
* Filter for boroughs in London only as that is the target market we are interested in only, this reduces the number of rows to 9,936.


```{r, include = FALSE}
data_monthly <- data_monthly_raw %>% 
  filter(borough_flag == 1) %>%
  mutate(year = as.numeric(substr(date,1,4))) %>% #to preserve the previous manipulations
  mutate(month = as.numeric(substr(date,6,7))) %>% #to preserve the previous manipulations
  mutate(houses_sold = ifelse(is.na(houses_sold), mean(houses_sold,na.rm=TRUE), houses_sold))

library(tidyverse)  
#checking for missing values
view(miss_var_summary(data_monthly))

```

Next, we explore the yearly data.

```{r, include = FALSE}
#importing the data from the yearly file
data_yearly_raw <- read.csv("1_data/housing_in_london_yearly_variables.csv", sep = ",", header = TRUE)

str(data_yearly_raw)
head(data_yearly_raw)

```

The yearly data is more generous, it contains more factors that could contribute to average house prices, e.g. life satisfaction, area size, population, salary, etc. As pointed out in the monthly data, we see that "code" and "area" are the common variables / columns among the two data files.


```{r}
#describe(data_yearly)
 
library(knitr)
library(kableExtra)
miss_var_summary(data_yearly_raw)
```
Like the monthly data, there seems to be some missing variables in the yearly data, most of which can be manipulated to populate some data. The field "life satisfaction" is almost 70% missing from the data, so we might choose to drop that field altogether. Before we do that, we look at the distribution of life satisfaction, area size, number of houses, and number of jobs to see why there are so many missing values.


```{r, warning = FALSE}

#introduce the year column for easy plotting and only filter for London boroughs
data_yearly <- data_yearly_raw %>% 
  filter(borough_flag == 1) %>%
  mutate(year = as.numeric(substr(date,1,4)))

p1 <- data_yearly %>% 
        ggplot(aes(year, life_satisfaction)) +
        geom_point(color = "blue") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p2 <- data_yearly %>% 
        ggplot(aes(year, area_size)) +
        geom_point(color = "dark green") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p3 <- data_yearly %>% 
        ggplot(aes(year, no_of_houses)) +
        geom_point(color = "maroon") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p4 <- data_yearly %>% 
        ggplot(aes(year, number_of_jobs)) +
        geom_point(color = "orange") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p5 <- data_yearly %>% 
        ggplot(aes(year, population_size)) +
        geom_point(color = "mistyrose4") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

p6 <- data_yearly %>% 
        ggplot(aes(year, median_salary)) +
        geom_point(color = "plum4") +
        theme(plot.margin=unit(c(0.25,0.5,0.25,0.5),"cm"))

#use gridExtra for arranging the graphs on one panel
#install.packages("gridExtra")
library(gridExtra)
#install.packages("grid")
library(grid)

grid.arrange(
  p1,
  p2,
  p3,
  p4,
  p5,
  p6,
  nrow = 3.5,
  top = "Why are there missing values from these variables?",
  bottom = textGrob(
    "filtered for areas in London boroughs",
    gp = gpar(fontface = 3, fontsize = 9),
    hjust = 1,
    x = 1
  )
)

```

The reason why life satisfaction has so many missing variables is because the data collection did not start until past 2010. While we may not use this in our machine learning techniques, we could still choose to separate the analysis into post-2010 with life satisfaction added as another factor.

For the other missing variables, we make the following observations:
* population size, number of jobs, and number of houses for 2019 is not populated
* area size is constant and only populated from 2001 to 2018
* median salary is populated since 1999


```{r, include = FALSE}
#don't need to include the output from here

#need the dplyr for data manipulation
#install.packages("dplyr")
library(dplyr)

data_yearly <- data_yearly_raw %>% 
  filter(borough_flag == 1) %>%
  mutate(year = as.numeric(substr(date,1,4))) %>% 
  mutate(month = as.numeric(substr(date,6,7)))  

#the following code populates area size for each borough according to its mean (constant number)
#also the same procedure for median salary
#install.packages("data.table")
library(data.table)
library(plyr); library(dplyr)
setDT(data_yearly)
data_yearly[, area_size := impute_mean(area_size), by = area][, median_salary := impute_mean(median_salary), by = area]
data_yearly %>%
    group_by(area) %>%
    mutate(
        area_size = impute_mean(area_size),
        median_salary = impute_mean(median_salary)
    )

as.data.frame(data_yearly)
```

The following data manipulations are performed to the yearly data:

* Similarly to monthly data, introduce two additional columns called "year" and "month" to represent the year and month of the data
* Replace the missing values of median salary with its mean values because the percentages missing are low; 
* Replace the missing values of area size with its the associated borough's area size as these area sizes do not change overtime - they have been set in stone since the beginning; and
* Filter for boroughs in London only as that is the target market we are interested in only, this reduces the number of rows to 693.

```{r, include = FALSE}
#checking for missing variables again after filtering and manipulating
miss_var_summary(data_yearly)
```
### Joining the two data files into one

The variable we are interested in predicting is the "average house price", which is contained in the monthly data. However, there are more attributes in the yearly data, so the final data table should be the yearly data plus the attributes from the monthly data. The yearly data has 693 observations, made up of data from 33 boroughs over 21 years.

```{r}

#checking that the number of observations are truly unique in the yearly data
#cat("number of unique rows in yearly data = ", nrow(unique(data_yearly[c("year","month","area")])), "\n") #693 rows, so yes, all year/month/area is unique

#use left join to join the monthly data onto the yearly data
#join on year, month, and area
data_join <- left_join(data_yearly, data_monthly, by = c("year", "month", "area"))

#do the same check for unique values in final dataset after join
#cat("number of unique rows in final data = ", nrow(unique(data_final[c("year","month","area")])), "\n")

```

A preview of the final data table is shown below.

```{r}
head(data_join)
```


\newpage
## Exploratory Data Analysis

The dataset now has average house prices for 33 boroughs since 1999 until 2019 (21 years), along with the following 9 attributes:
```{r}
#create matrix with 3 columns
tab <- matrix(c("median salary", "-",
                "mean salary", "-",
                "percentage of recycling houses", "-",
                "population size", "data collected until 2018 only",
                "number of jobs", "(data collected until 2018 only)",
                "number of houses sold", "(data collected until 2018 only)",
                "area size", "(data collected from 2001 only)",
                "number of crimes", "(data collected from 2001 only)",
                "life satisfaction", "(data collected from 2011 only)"), 
              ncol=2, byrow=TRUE)

#define column names and row names of matrix
colnames(tab) <- c("Factor or attribute", "Comments on missing variables")
rownames(tab) <- c("1.", "2.", "3.", "4.", "5.", "6.", "7.", "8.", "9.")

#convert matrix to table 
tab <- as.table(tab)

#view table 
tab
```
The 33 boroughs are shown below. 

```{r, include=FALSE}
print(unique((data_join$area)))

```

The graph below shows how average house prices have changed from 1999 to 2019. Average house prices have increased for all areas in general, although some areas have had more dramatic increases compared to others. There is a slight dip in average house prices around 2008/2009, which could be due to the GFC. An indicator that is closely related to the GFC is mean and median salary. We will have a look at the correlation of salary with average house prices later on.

```{r}

data_join %>% 
  ggplot(aes(year,average_price, col=area)) +
  geom_line() +
  theme(legend.position = "bottom") + 
  theme(legend.key.height = unit(0.3, "cm")) +
  theme(plot.title = element_text(face="bold", hjust=0.5)) +
  theme(plot.subtitle = element_text(hjust=0.5)) +
  theme(plot.margin=unit(c(0.2,0.5,0.1,0.5),"cm")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Average House Prices from 1999 to 2019 by Area", subtitle = "Overall increase except in year 2008", x ="Year", y="Average house price (£)") +
  scale_y_continuous(
  labels = scales::comma_format(big.mark = ',',
                                decimal.mark = '.')) +
  scale_x_continuous(breaks = c(1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)) +
  geom_vline(xintercept=2008, linetype = "dashed", col = "black")

  
```


```{r, include = FALSE}

data_join %>% 
  ggplot(aes(year,average_price, col = area)) +
  geom_line(show.legend = FALSE) +
  facet_wrap(~ area)

```

The diagram below shows that Kensington and Chelsea and Westminister have the highest average house prices in London, followed by Camden, Hammersmith, and London. Area seems to be a strong indicator to whether the average house price is high or not.

```{r}

#install.packages("scales")
library(scales)
data_join %>% 
  ggplot(aes(x=reorder(area, -average_price), y=average_price, col = area)) +
  #ggplot(aes(x=area, y=average_price, col = area)) +
  geom_boxplot(show.legend = FALSE) +
  theme(axis.text.x = element_text(angle=90, hjust=1)) +
  theme(plot.title = element_text(face="bold", hjust=0.5)) +
  theme(plot.subtitle = element_text(hjust=0.5)) +
  theme(plot.margin=unit(c(0.2,0.5,0.1,0.5),"cm")) +
  labs(title="Distribution of average house prices", subtitle = "How are average prices different in each area?", x ="Area", y="Average house price (£)") +
  scale_y_continuous(
  labels = scales::comma_format(big.mark = ',',
                                decimal.mark = '.'))
  
```


\newpage
## Factors Contributing to House Prices

We know that area (i.e. location of the house) is a very important factor. However, area or location is not an attribute on its own, but rather the makeup of several attributes in the data. For example:

* number of crimes is a proxy for safety, 
* median and mean salary is a proxy for income and income inequality
* population size and area size is a proxy for population density
* percentage of recylcing is a proxy for social policy

_Location_ (and the above attributes) drive the demand for housing in a particular area, and therefore the average house price. The map below demonstrates the importance of location - areas around Kensington and Chelsea have higher average house prices than the ones further away.

```{r, include = FALSE}

#installing packages required for showing map 

#install.packages("tmap")
#install.packages("sf")
#install.packages("raster")
#install.packages("spData")
#install.packages("spDataLarge", repos = "https://nowosad.github.io/drat/", type = "source")
#install.packages("rgdal")

library(tidyverse) #for view function
library(rgdal) #for reading geo file
#library(spData) #geopackage file, but no longer needed because loaded London geo data separately
library(tmap) #for plotting maps
library(sf) #for plotting maps
#library(spDataLarge) #geopackage file, but no longer needed because loaded London geo data separately
library(raster)

london <- readOGR("1_data/London_Borough_Excluding_MHW/London_Borough_Excluding_MHW.shp")
view(london)

#could not use join function so use merge function  
#right_join(data_join %>% dplyr::select(average_price), london, by = "GSS_CODE")
data_join_map <- merge(london,
                       data_join %>% dplyr::select(code.x, average_price, year) %>% dplyr::filter(year==2018), #get one year's data 2018, to represent the typical average house price, otherwise there will be non unique values
                       by.x="GSS_CODE",
                        by.y="code.x")

view(data_join_map)

```

```{r}
#install.library("RColourBrewer")
library(RColorBrewer)

my.palette <- brewer.pal(n = 9, name = "Reds") #creates colour template

spplot(data_join_map, "average_price",
       col.regions = my.palette,
       cuts = 8,
       main = "Greater London Area",
       sub = "Average House Price")
       
```

Since majority of the data are complete as at 2018, we will only consider analysis on the data from 1999 to 2018, and then separately with life satisfaction from 2011 to 2018. 

```{r, include=FALSE}
#need the corrplot package for correlation visualisation
#install.packages("corrplot")
#install.packages("naniar")

library(corrplot) #for correlation
library(dplyr) #for mutate and select
library(naniar) #for missing variable summary
library(tidyverse) #for view function

#first need to get out the numerical variables only
data_corr <- data_join %>% 
  filter(year>2010, year<2019) %>% 
  mutate(average_price = as.numeric(average_price), 
         median_salary = as.numeric(median_salary), 
         mean_salary = as.numeric(mean_salary),
         life_satisfaction = as.numeric(life_satisfaction), 
         recycling_pct = as.numeric(recycling_pct), 
         population_size = as.numeric(population_size), 
         number_of_jobs = as.numeric(number_of_jobs), 
         area_size = as.numeric(area_size), 
         no_of_crimes = as.numeric(no_of_crimes), 
         no_of_houses = as.numeric(no_of_houses),
         houses_sold = as.numeric(houses_sold)) %>%
  dplyr::select(average_price, 
         median_salary, 
         mean_salary,
         life_satisfaction, 
         recycling_pct, 
         population_size, 
         number_of_jobs, 
         area_size, 
         no_of_crimes, 
         no_of_houses,
         houses_sold) 

str(data_corr) #all numerical
view(miss_var_summary(data_corr)) #need to remove the missing values otherwise correlation matrix does not work

#since there are not many (only 10 rows) missing, we remove the NAs
data_corr_complete <- na.omit(data_corr)
view(miss_var_summary(data_corr_complete)) #no missing variables

#correlation matrix
corr <- cor(data_corr_complete)

```


```{r}
#combine the correlation matrix with significance test
#install.packages("Hmisc")
library(Hmisc)
cor_5 <- rcorr(as.matrix(data_corr_complete))
M <- cor_5$r 
p_mat <- cor_5$P #p-value

#then plot the correlation matrix

corrplot(M, method = "circle", type = "lower",  #type of correlation visualisation, only want one half of it
         tl.col = "darkblue", tl.srt = 65, #change colour of text and slant it
         tl.cex = 0.7, cl.cex = 0.7,
         p.mat = p_mat, sig.level = 0.001, insig = "blank", #remove those insignificant ones, use 0.001 as the level of significance 
         mar=c(0,0,3,0),
         title = "How do the variables relate average house price and each other?")

```
From the correlation matrix above, we make the following observations:

* Average price is positively correlated with salary, number of jobs, life satisfaction (unsurprising);
* Number of jobs in the area also increase the average house price (possibly because commute is convenient);
* Number of crimes increase where there is higher number of jobs so average house prices also increase where there are higher number of crime; and
* Average house price is lower when population size and area size are bigger. Area size is negatively correlated with number of jobs, which implies that these areas have large lands, but fewer job opportunities, hence lower average house prices.

For our models, we split the data into train and test data using a 90%/10% split and make the variables numeric.

```{r, include = FALSE}

#first separate out the factors used for analyses ane make them into numeric values
#this is repeating the process used for correlation matrix
#only use data between 2001 and 2018 because that is when most data is complete

data_NN <- data_join %>% 
  filter(year>2000, year<2019) %>% 
  mutate(average_price = as.numeric(average_price), 
         median_salary = as.numeric(median_salary), 
         mean_salary = as.numeric(mean_salary),
         #life_satisfaction = as.numeric(life_satisfaction), 
         recycling_pct = as.numeric(recycling_pct), 
         population_size = as.numeric(population_size), 
         number_of_jobs = as.numeric(number_of_jobs), 
         area_size = as.numeric(area_size), 
         no_of_crimes = as.numeric(no_of_crimes), 
         no_of_houses = as.numeric(no_of_houses),
         houses_sold = as.numeric(houses_sold)) %>%
  dplyr::select(average_price, 
         median_salary, 
         mean_salary,
         #life_satisfaction, 
         recycling_pct, 
         population_size, 
         number_of_jobs, 
         area_size, 
         no_of_crimes, 
         no_of_houses,
         houses_sold) 

view(miss_var_summary(data_NN)) #some variables missing, remove them because there are not many
data_NN_complete <- na.omit(data_NN) #remove NAs
view(miss_var_summary(data_NN_complete)) 

#now we split into train and test
set.seed(101) #to fix the randomness

#set 70% for train 30% for test
trainsampleindex <- sample(1:nrow(data_NN_complete), 0.9*nrow(data_NN_complete))

#create the train and test below
train <- data_NN_complete[trainsampleindex,]
test <- data_NN_complete[-trainsampleindex,]


```

### Modelling using Neural Network and Linear Models

The first model chosen to fit the train data is a neural network (NN) model, which are nonlinear supervised regression models.

We check to see the distribution of average house price in the training data. The diagram below shows that average house prices can have quite large scale and ranges, compared to the other attributes in the model.

```{r}
#plot histogram to see if we need to normalise the data first for NN

train %>% ggplot(aes(average_price)) +
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
  ggtitle("Average price distribution of London houses") +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5)) +
  theme(plot.margin=unit(c(0.2,0.5,0.1,0.5),"cm")) +
  labs(x = "Average House Price (£)", y = "Density") +
  scale_x_continuous(breaks = c(250000, 500000, 750000, 1000000, 1250000, 1500000),
                     labels = scales::comma_format(big.mark = ',',
                                decimal.mark = '.')) + 
  scale_y_continuous(labels = scales::number_format(accuracy = 0.000001,
                                 decimal.mark = '.'))

  
```
Therefore we scale the features in the training data before applying any model fit to it.

The following diagram shows how a neural network with two hidden layers (6 and 4 nodes) predict the average house price.

```{r}

#we need to scale data
max.value <- apply(data_NN_complete, 2, max) 
min.value <- apply(data_NN_complete, 2, min)

#scale inputs and output to [0,1] intervals using min and max
scaled <- as.data.frame(scale(data_NN_complete, center = min.value, scale = max.value - min.value)) 

train_scaled <- scaled[trainsampleindex,]
test_scaled <- scaled[-trainsampleindex,]

####
### this normal scaling is not used now
#train_scaled <- scale(train)
#Use means and standard deviations from training set to normalize test set
#col_means_train_scaled <- attr(train_scaled, "scaled:center") 
#col_stddevs_train_scaled <- attr(train_scaled, "scaled:scale")

#test_scaled <- as.data.frame(scale(test, center = col_means_train_scaled, scale = col_stddevs_train_scaled))
#train_scaled <- as.data.frame(scale(train, center = col_means_train_scaled, scale = col_stddevs_train_scaled))


```

```{r}
#install.packages("neuralnet")
library(neuralnet)
library(dplyr)
#build a neural network

set.seed(180)
n <- names(train_scaled)
f <- as.formula(paste("average_price ~", paste(n[!n %in% "average_price"], collapse = " + ")))
nn <- neuralnet(f,data=train_scaled, hidden=c(6,4),act.fct = 'tanh',linear.output=T, stepmax=1e7)

plot(nn, rep="best")

```
The diagram below shows how the NN model fits to its test data.

```{r}

#rescaling should be in reference to the mean and std dev i.e. the min and max
pr.nn <- compute(nn, test_scaled[,1:10])
pr.nn.rescaled <- pr.nn$net.result * 
  (max(data_NN_complete$average_price)-min(data_NN_complete$average_price)) + min(data_NN_complete$average_price)
test.rescaled <- test_scaled$average_price * 
  (max(data_NN_complete$average_price)-min(data_NN_complete$average_price)) + min(data_NN_complete$average_price)

#plotting the predicted NN with the test data
#png("2_images/nn.png") #save this for reproducing in model evaluation later

par(mfrow=c(1,2))
plot(test$average_price,pr.nn.rescaled,col='red',main='predicted NN vs real',pch=18,cex=0.7, xlab = "Average house price (£)", ylab = "Predicted")
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')



```

```{r}
#calculating the MSE for the NN
MSE.nn <- sum((test.rescaled - pr.nn.rescaled)^2)/nrow(test_scaled)
cat("RMSE for NN model =", sqrt(MSE.nn), "\n")

```
We will try fitting the train data with a linear model to see how the NN compares.

```{r, warning = FALSE}

#use a GLM for comparison against the NN
library(boot)
set.seed(2)

lm.fit <- glm(average_price~., data=train)
lm.pred <- predict(lm.fit, test)
summary(lm.fit)

```

```{r, include = FALSE}
#to calculate pseudo R square for comparison with the NN
#install.packages("rcompanion")
library(rcompanion)
nagelkerke(lm.fit)
```
The linear model has a higher root mean square error compared to the NN model. This means the model accuracy is less so than the NN. We also show the cross validation error below so we can compare it to the NN. 

```{r}

MSE.lm <- sum((lm.pred - test$average_price)^2)/nrow(test)
cat("RMSE for linear model =", sqrt(MSE.lm), "\n")

library(boot)
lm.fullfit <- glm(average_price~.,data=data_NN_complete)
cat("10 fold cross valiation error for GLM =", cv.glm(data_NN_complete,lm.fullfit,K=10)$delta[1], "\n")

```
```{r}
#plot both of the NN and GLM predicted results versus the real
#png("2_images/glmnn.png") #save this for reproducing in model evaluation later

plot(test$average_price,pr.nn.rescaled,col='red',main='Predicted vs Real for NN and GLM',pch=18,cex=0.7, xlab = "Average house price (£)", ylab = "Predicted")
points(test$average_price,lm.pred,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','GLM'),pch=18,col=c('red','blue'))

```

```{r}

set.seed(450)
cv.error <- NULL
k <- 10
library(plyr) 
pbar <- create_progress_bar('text')
pbar$init(k)
for(i in 1:k){
    index <- sample(1:nrow(data_NN_complete),round(0.9*nrow(data_NN_complete)))
    train.cv <- scaled[index,]
    test.cv <- scaled[-index,]
    nn <- neuralnet(f,data=train.cv,hidden=c(6,4),linear.output=T)   
    pr.nn <- compute(nn,test.cv[,1:10])
    pr.nn <- pr.nn$net.result*(max(data_NN_complete$average_price)-min(data_NN_complete$average_price))+min(data_NN_complete$average_price)   
    test.cv.r <- (test.cv$average_price)*(max(data_NN_complete$average_price)-min(data_NN_complete$average_price))+min(data_NN_complete$average_price)   
    cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv)    
    pbar$step()
}

```
```{r}

cat("NN cross validation error =", cv.error, "\n")
cat("Mean NN cross validation error =", mean(cv.error), "\n")

```
```{r}
boxplot(cv.error,xlab='MSE CV',col='light green',
        border='black',names="CV error (MSE)",
        main ="CV error (MSE) for NN",horizontal=TRUE) 

```
Mean CV error for NN is 6bn, which is lower than GLM CV error, 17bn, although there seems to be a certain degree of variation in the MSEs of the cross validation. 

The accuracy of model (proxied by root mean square error, RMSE) for the NN machine learning applied in our model was aprox. 62,000 compared to GLM's MSE of approx. 100,000 This shows that the neural network was a better predictor of average house prices compared to GLM.

We look at another model to see if it is a better fit.

### Modelling using Decision Tree

Decision trees or regression trees are also machine learning models that can be used to predict continuous variables like average house prices.

We use the same joined data we produced after data preparation, but allow for the variable, life satisfaction, to be included. The decision tree model is less stringent than the NN model (which did not allow missing values), hence we decide to bring in life satisfaction to see if it makes the model more accurate.

```{r}
require(rpart)
require(rpart.plot)

#prepare the data for tree regression, using the same data_join set
#no need to remove the life satisfaction because NAs will not stop the tree process
#see if this is a better fit compared to the previous NN
data_tree <- data_join %>% 
  filter(year>2000, year<2019) %>% 
  mutate(average_price = as.numeric(average_price), 
         median_salary = as.numeric(median_salary), 
         mean_salary = as.numeric(mean_salary),
         life_satisfaction = as.numeric(life_satisfaction), 
         recycling_pct = as.numeric(recycling_pct), 
         population_size = as.numeric(population_size), 
         number_of_jobs = as.numeric(number_of_jobs), 
         area_size = as.numeric(area_size), 
         no_of_crimes = as.numeric(no_of_crimes), 
         no_of_houses = as.numeric(no_of_houses),
         houses_sold = as.numeric(houses_sold)) %>%
  dplyr::select(average_price, 
         median_salary, 
         mean_salary,
         life_satisfaction, 
         recycling_pct, 
         population_size, 
         number_of_jobs, 
         area_size, 
         no_of_crimes, 
         no_of_houses,
         houses_sold) 

set.seed(1080) #to fix the randomness

#set 90% for train 10% for test
trainsampleindex_tree <- sample(1:nrow(data_tree), 0.9*nrow(data_tree))

#create the train and test below
train_tree <- data_tree[trainsampleindex_tree,]
test_tree <- data_tree[-trainsampleindex_tree,]

```
The following is the first stab at the decision tree before any action to improve accuracy.

```{r}
#start with a big tree and prune later
tree <- rpart(average_price ~ .,
                         data=train_tree,
                         method="anova", #not Poisson nor discrete so choose anova
                         control = rpart.control(xval=10,cp=0.0000001))
rpart.plot(tree,
           main = "Decision Tree before pruning")

```
```{r}
#introduce new function for calculating MSE for regression tree
rmse_reg <- function(model_obj, testing = NULL, target = NULL) {
  #Calculates rmse for a regression decision tree
  #Arguments:
    # testing - test data set
    # target  - target variable (length 1 character vector)
  yhat <- predict(model_obj, newdata = test_tree)
  actual <- testing[[target]]
  sqrt(mean((yhat-actual)^2))
}
```

```{r}
cat("RMSE of regression tree =", rmse_reg(tree, test_tree, "average_price"), "\n")
```
If we grow an overly complex tree as in the tree diagram above, there tends to be overfitting to the training data, which results in poor generalisation performance. To find the balance of depth and complexity of the tree, there are two approaches - pruning and early stopping.

We explore the pruning option. First, we find the optimal complexity parameter with the help of the chart below.

```{r}
#find the optimal cp value using the plot below

plotcp(tree,minline=TRUE,
       col = "red")
```
From the above diagram, we can see optimal complexity parameter is roughly 0.01. We prune the tree to return a smaller optimal tree. We calculate the optimal complexity parameter by using the 1SD rule as well to double check.

```{r}
cp.select <- function(big.tree){
  min.x <- which.min(big.tree$cptable[, 4])
  for(i in 1:nrow(big.tree$cptable)){
    if(big.tree$cptable[i, 4] < big.tree$cptable[min.x, 4] 
       + big.tree$cptable[min.x, 5]){
      return(big.tree$cptable[i, 1])
    }
  }
}
CP_select <- cp.select(tree)
cat("Optimal complexity parameter = ", CP_select, "\n")
```
The tree with the optimal cp value has fewer leaves now.
It shows that salary information and number of jobs are the most influential predictors of average house price. This is a similar finindg with our correlation matrix from the previous section.

```{r}
tree_pruned <- prune(tree, cp = CP_select)
rpart.plot(tree_pruned,
           main = "Decision Tree after Pruning")
```
The diagram below shows how well the pruned decision tree fits its test data. 

```{r dev = "png"}

yhat <- predict(tree_pruned, newdata = test_tree)
actual <- test_tree$average_price

#png("2_images/nntreepruned") #save this for reproducing in model evaluation later

plot(yhat, actual, xlab = "Predicted", ylab = "Actual", main = "Predicted vs Actual after pruning")
abline(0,1)

```

```{r}
#previously using this, use the next one to calculate sq root of MSE instead
#cat("MSE of tree_pruned =", sum(((yhat-actual)^2))/nrow(test_tree), "\n")


#calculate and print the MSE for tree after pruning
cat("RMSE of regression tree after pruning =", rmse_reg(tree_pruned, test_tree, "average_price"), "\n")
```

The MSE is slightly higher than the previous tree, but still higher than that of the neural network model, which was approx. 62,000. 

We use the CARET package to train the tree data to find the best fit using the minimum of RMSE.

```{r, warning = FALSE}

#need CARET package for traincontrol function
#install.packages("caret")
library(caret)

trgrid <- expand.grid(cp = seq(0,0.4,0.01))
tc <- trainControl("cv",10)
set.seed(2)
train.rpart <- caret::train(x = train_tree[,-"average_price"], #caret:: because of mlr conflict and me using same object name
                     y = train_tree$average_price,
                     method = "rpart",
                     tuneLength = 50,
                     trControl = tc,
                     tuneGrid = trgrid )
train.rpart

```

We calculate the RMSE using CARET
```{r}
rmse_reg(train.rpart, test_tree, "average_price")
```
The MSE has greatly reduced to less than 4,000, even more effective than the previous NN model. However, the diagram below shows that there are many leaves compared to the unpruned tree. 

```{r}

require(rpart.plot)
rpart.plot(train.rpart$finalModel,
           main = "Decision Tree using Caret")

```
The new model resulted in a very large tree as pictured above. There is very high chance that it is overfitting the model.

The plot below shows the predicted versus actual average house prices using the CARET tree model. While it reduces the MSE drastically, it is not recommended to use this approach for predicting average house prices.

```{r}
yhat2 <- predict(train.rpart$finalModel, newdata = test_tree)
actual2 <- test_tree$average_price
plot(yhat2, actual2, xlab = "Predicted", ylab = "Actual", main = "Predicted vs Actual using CARET")
abline(0,1)
```

\newpage

## Evaluation of Models

The accuracy of the models (measured by root mean squared error) are summarised below:

```{r}

cat("RMSE for linear model =", sqrt(MSE.lm), "\n")
cat("RMSE for NN model =", sqrt(MSE.nn), "\n")
cat("RMSE of regression tree =", rmse_reg(tree, test_tree, "average_price"), "\n")
cat("RMSE of regression tree after pruning =", rmse_reg(tree_pruned, test_tree, "average_price"), "\n")
cat("RMSE using decision tree with CARET =", rmse_reg(train.rpart, test_tree, "average_price"), "\n")

```

Neural Network model did the best job in predicting average house prices in London. Multiple Linear Regression and Decision Tree was not as good at predicting the average house prices. Decision tree without pruning and using CARET resulted in over-fitted models that are no good for predictions.

```{r}
#reproducing the code used to produce NN predicted vs. real
par(mfrow=c(1,2))
plot(test$average_price,pr.nn.rescaled,col='red',main='Predicted NN vs real',pch=18,cex=0.7, xlab = "Average house price (£)", ylab = "Predicted")
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')

```
\newpage

## Conclusion

This report demonstrated how machine learning has applications in many fields, such as predicting the value of house prices for many interested parties like investors, banks, insurers, the government, and many more. Managing the risk of fluctuations in house prices helps manage the financial risks for these stakeholders due to their asset values, loan values, insurance payout (property losses), and affordability of house prices.

First, we introduced the datasets and made sure they are prepped for the analysis e.g. removing missing variables. 

Then, we looked at the data and the story it tells us through visualisation (maps, time series, correlation). We discovered that:

* The area (or location) matters, but it is the makeup of other attributes that drive whether or not the area is desirable, and hence average house prices.
* Average house prices have increased for all areas in general, although some areas have had more dramatic increases compared to others. There is a slight dip in average house prices around 2008/2009, which could be due to the GFC.
* Average price is positively correlated with salary, number of jobs, life satisfaction (unsurprising);
* Number of jobs in the area also increase the average house price (possibly because commute is convenient);
* Number of crimes increase where there is higher number of jobs so average house prices also increase where there are higher number of crime; and
* Average house price is lower when population size and area size are bigger. Area size is negatively correlated with number of jobs, which implies that these areas have large lands, but fewer job opportunities, hence lower average house prices.

We first tried fitting a linear model, but discovered that the accuracy of such a linear model is quite poor compared to other machine learning techniques. So we fitted different types of machine learning models to the training data to see how well the models predict average house prices compared to the actual data (from test dataset). We explored a simple Neural Network with a few hidden layers (but not very deep), we can see that the accuracy of the prediction improved compared to the linear model.

We then tried decision trees to see if they can predict better than NN and linear models. The advantage of using a decision tree is that trees are very easy to explain and interpret. However, we concluded that from our model fitting and evaluation above, the prediction accuracy of a decision tree (even after pruning) is not as good as NN regression model. 

Finally, we tried letting the decision tree model streamline the model building and evaluation process (via the CARET package). Even though the resulted model accuracy was very low (lowest amongst all models), it was obvious that the model was over-fitted, which means that decision tree model learnt the training data too well and cannot generalise from data it has not seen before. We do not recommend using that decision tree model for predicting average house prices.

## Limitations and Potential Future Work

The dataset is focused on London's average house prices within boroughs, with data quite heavily focused on financial propensity of an area e.g. salary and number of jobs. There is no data on the economic activity of London (e.g. GDP or interest rates) which could definitely influence average house prices too. 

The analysis focused on whether average house prices can be predicted from existing data, but it does not account for one-off systemic events such as the GFC (and Brexit later on). This is where the economic activity of London might help explain the average house prices better than only relying on area attributes.

The NN model was shown to be the best predictor amongst all the models explored in the report. It could be improved by using more layers and deeper layers, although it can become computationally very long and complex. 

\newpage
## Appendix: R Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```

